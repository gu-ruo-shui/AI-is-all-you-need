{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 大模型简介\n",
    "- 将互联网上的资料压缩在 LLM 中形成概率权重, 然后根据输入(prompt + output)预测下一个单词\n",
    "- output[n] = f(prompt + output[:n-1])\n",
    "- 入门文章\n",
    "  - https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "# 大模型基本参数\n",
    "- 基本对话\n",
    "- top_logprobs\n",
    "- context size\n",
    "- logit_bias presence_penalty frequency_penalty \n",
    "- structure output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T12:37:45.257840Z",
     "start_time": "2025-04-11T12:37:44.562405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-v3-0324\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "model = os.getenv(\"OPENAI_MODEL\")\n",
    "# print(client.base_url)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:44:55.101947Z",
     "start_time": "2025-04-07T14:44:51.804059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6, 7, 8, 9, 10'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non stream\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:16:45.565992Z",
     "start_time": "2025-04-08T15:16:43.889249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'q: what is the capital of fracnce?\\n a:'}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=2\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:16:47.276377Z",
     "start_time": "2025-04-08T15:16:47.256683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-19SN4qBjCbt6cVqJrPR4Pp54RDm354l0',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'The',\n",
       "      'bytes': [84, 104, 101],\n",
       "      'logprob': -6.630610641877865e-06,\n",
       "      'top_logprobs': [{'token': 'The',\n",
       "        'bytes': [84, 104, 101],\n",
       "        'logprob': -6.630610641877865e-06},\n",
       "       {'token': 'a', 'bytes': [97], 'logprob': -12.250006675720215}]},\n",
       "     {'token': ' capital',\n",
       "      'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' capital',\n",
       "        'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'capital',\n",
       "        'bytes': [99, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': -19.75}]},\n",
       "     {'token': ' of',\n",
       "      'bytes': [32, 111, 102],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' of',\n",
       "        'bytes': [32, 111, 102],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' city',\n",
       "        'bytes': [32, 99, 105, 116, 121],\n",
       "        'logprob': -21.25}]},\n",
       "     {'token': ' France',\n",
       "      'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' France',\n",
       "        'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'France',\n",
       "        'bytes': [70, 114, 97, 110, 99, 101],\n",
       "        'logprob': -17.75}]},\n",
       "     {'token': ' is',\n",
       "      'bytes': [32, 105, 115],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' is',\n",
       "        'bytes': [32, 105, 115],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' هو',\n",
       "        'bytes': [32, 217, 135, 217, 136],\n",
       "        'logprob': -21.25}]},\n",
       "     {'token': ' Paris',\n",
       "      'bytes': [32, 80, 97, 114, 105, 115],\n",
       "      'logprob': -1.1472419600977446e-06,\n",
       "      'top_logprobs': [{'token': ' Paris',\n",
       "        'bytes': [32, 80, 97, 114, 105, 115],\n",
       "        'logprob': -1.1472419600977446e-06},\n",
       "       {'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': -13.750000953674316}]},\n",
       "     {'token': '.',\n",
       "      'bytes': [46],\n",
       "      'logprob': -5.512236498361744e-07,\n",
       "      'top_logprobs': [{'token': '.',\n",
       "        'bytes': [46],\n",
       "        'logprob': -5.512236498361744e-07},\n",
       "       {'token': '.\\n', 'bytes': [46, 10], 'logprob': -14.500000953674316}]}],\n",
       "    'refusal': None},\n",
       "   'message': {'content': 'The capital of France is Paris.',\n",
       "    'role': 'assistant'}}],\n",
       " 'created': 1744125415,\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': 'fp_86d0290411',\n",
       " 'usage': {'completion_tokens': 8, 'prompt_tokens': 20, 'total_tokens': 28}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:16:49.891433Z",
     "start_time": "2025-04-08T15:16:49.871435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> The, <span style='color: darkorange'>logprobs:</span> -6.630610641877865e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> a, <span style='color: darkorange'>logprobs:</span> -12.250006675720215, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "\n",
    "top_two_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "html_content = \"\"\n",
    "for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "    html_content += (\n",
    "        f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "        f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "        f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "    )\n",
    "display(HTML(html_content))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context size\n",
    "- 产生的 token 越多, 模型运行过程中要的显存也就越大.\n",
    "- tokenizer_config.json 中 model_max_length (context size)\n",
    "    - context size = input (prompt) + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:32:31.224514Z",
     "start_time": "2025-04-10T13:32:21.191930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这座山巍峨耸立，宛如大地的守护者，峰顶常年被白雪覆盖，宛如一顶银色的 crown。山体由坚硬的岩石构成，带有复杂的纹理和色彩，映射出岁月的痕迹。清晨时分，阳光透过薄雾，给山脊镶上金边，显得格外神秘。山脚下，溪水潺潺，潺潺而流，鱼儿在水中嬉戏，周围的树木郁郁葱葱，仿佛在向游客诉说着这座山的古老传说。\n",
      "\n",
      "每到秋天，山上的树叶变幻出绚丽的色彩，红、黄、橙相互交织，犹如一幅美丽的画卷。登顶之路蜿蜒曲折，需经历艰难的攀登，但当你站在山顶，俯瞰四周，壮丽的景色顿时令你心旷神怡。远方的群山连绵起伏，仿佛在向你讲述自然的伟大与神秘。这座山，不仅是大自然的杰作，更是人们心灵深处的向往和归宿。\n",
      "\n",
      "\n",
      "这座山巍峨雄伟，屹立在天地之间。山体披着厚厚的绿毯，苍翠欲滴的森林覆盖其间，让人仿佛置身于一片天然氧吧。阳光透过树梢洒下斑驳陆离的光影，鸟儿鸣唱声此起彼伏，为宁静的大自然增添了几分生机。\n",
      "\n",
      "随着高度逐渐增加，可以看到层层梯田和溪流蜿蜒而下，如丝带一般环绕四周。在山顶俯瞰，更是壮观无比：群峰耸立、云海翻腾，让人心旷神怡。有时薄雾缭绕，就像给这座独特的地方蒙上了一层神秘面纱。\n",
      "\n",
      "这里不仅是登高望远之地，也是许多珍稀动物与植物的天堂，是探险爱好者和摄影师们梦寐以求之所。在这样的环境中，人们能感受到大自然赋予的一种震撼力量，也让自己的内心得到洗礼与升华。这是一处让人与自然融为一体、沉浸于纯粹美好的精神乐园。\n"
     ]
    }
   ],
   "source": [
    "content = '200个字描述一座山'\n",
    "\n",
    "resp1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content}\n",
    "    ],\n",
    ")\n",
    "\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content}\n",
    "    ],\n",
    "    presence_penalty=2,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "\n",
    "print(resp1.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "print(resp2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer & logit_bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:49:06.711397Z",
     "start_time": "2025-04-10T12:49:06.414398Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:49:10.728704Z",
     "start_time": "2025-04-10T12:49:10.717708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12650]\n",
      " Paris\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(' Paris'))\n",
    "print(tokenizer.decode([12650]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:49:16.067347Z",
     "start_time": "2025-04-10T12:49:14.592055Z"
    }
   },
   "outputs": [],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'q: what is the capital of fracnce?\\n a:'}\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=2,\n",
    "    temperature=0,\n",
    "    logit_bias={12650:-100, 72782:-100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:07:29.903072Z",
     "start_time": "2025-04-09T14:07:29.891076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is باريس (in French: \"Pariser\").'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:07:32.034066Z",
     "start_time": "2025-04-09T14:07:32.018069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-r93AVey3euLTcvr7UAr4PP54RDM72tu0',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'The',\n",
       "      'bytes': [84, 104, 101],\n",
       "      'logprob': -3.8889261304575484e-06,\n",
       "      'top_logprobs': [{'token': 'The',\n",
       "        'bytes': [84, 104, 101],\n",
       "        'logprob': -3.8889261304575484e-06},\n",
       "       {'token': 'a', 'bytes': [97], 'logprob': -12.875003814697266}]},\n",
       "     {'token': ' capital',\n",
       "      'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' capital',\n",
       "        'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'capital',\n",
       "        'bytes': [99, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': -19.75}]},\n",
       "     {'token': ' of',\n",
       "      'bytes': [32, 111, 102],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' of',\n",
       "        'bytes': [32, 111, 102],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' city',\n",
       "        'bytes': [32, 99, 105, 116, 121],\n",
       "        'logprob': -21.125}]},\n",
       "     {'token': ' France',\n",
       "      'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' France',\n",
       "        'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'France',\n",
       "        'bytes': [70, 114, 97, 110, 99, 101],\n",
       "        'logprob': -17.125}]},\n",
       "     {'token': ' is',\n",
       "      'bytes': [32, 105, 115],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' is',\n",
       "        'bytes': [32, 105, 115],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' هو',\n",
       "        'bytes': [32, 217, 135, 217, 136],\n",
       "        'logprob': -21.375}]},\n",
       "     {'token': ' باريس',\n",
       "      'bytes': [32, 216, 168, 216, 167, 216, 177, 217, 138, 216, 179],\n",
       "      'logprob': -14.375003814697266,\n",
       "      'top_logprobs': [{'token': ' Paris',\n",
       "        'bytes': [32, 80, 97, 114, 105, 115],\n",
       "        'logprob': -3.7697225252486533e-06},\n",
       "       {'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': -13.000003814697266}]},\n",
       "     {'token': ' (',\n",
       "      'bytes': [32, 40],\n",
       "      'logprob': -0.09665730595588684,\n",
       "      'top_logprobs': [{'token': ' (',\n",
       "        'bytes': [32, 40],\n",
       "        'logprob': -0.09665730595588684},\n",
       "       {'token': '.', 'bytes': [46], 'logprob': -2.7216572761535645}]},\n",
       "     {'token': 'in',\n",
       "      'bytes': [105, 110],\n",
       "      'logprob': -10.625136375427246,\n",
       "      'top_logprobs': [{'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': -0.00013643606507685035},\n",
       "       {'token': 'pron',\n",
       "        'bytes': [112, 114, 111, 110],\n",
       "        'logprob': -9.875136375427246}]},\n",
       "     {'token': ' French',\n",
       "      'bytes': [32, 70, 114, 101, 110, 99, 104],\n",
       "      'logprob': -2.1362416744232178,\n",
       "      'top_logprobs': [{'token': ' Arabic',\n",
       "        'bytes': [32, 65, 114, 97, 98, 105, 99],\n",
       "        'logprob': -0.13624174892902374},\n",
       "       {'token': ' French',\n",
       "        'bytes': [32, 70, 114, 101, 110, 99, 104],\n",
       "        'logprob': -2.1362416744232178}]},\n",
       "     {'token': ':',\n",
       "      'bytes': [58],\n",
       "      'logprob': -1.4696030616760254,\n",
       "      'top_logprobs': [{'token': ')',\n",
       "        'bytes': [41],\n",
       "        'logprob': -0.7196031212806702},\n",
       "       {'token': ':', 'bytes': [58], 'logprob': -1.4696030616760254}]},\n",
       "     {'token': ' \"',\n",
       "      'bytes': [32, 34],\n",
       "      'logprob': -8.500207901000977,\n",
       "      'top_logprobs': [{'token': ' Paris',\n",
       "        'bytes': [32, 80, 97, 114, 105, 115],\n",
       "        'logprob': -0.00020747410599142313},\n",
       "       {'token': ' \"', 'bytes': [32, 34], 'logprob': -8.500207901000977}]},\n",
       "     {'token': 'Par',\n",
       "      'bytes': [80, 97, 114],\n",
       "      'logprob': -11.87501049041748,\n",
       "      'top_logprobs': [{'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': -1.0683535037969705e-05},\n",
       "       {'token': 'Par',\n",
       "        'bytes': [80, 97, 114],\n",
       "        'logprob': -11.87501049041748}]},\n",
       "     {'token': 'iser',\n",
       "      'bytes': [105, 115, 101, 114],\n",
       "      'logprob': -4.711602210998535,\n",
       "      'top_logprobs': [{'token': 'i',\n",
       "        'bytes': [105],\n",
       "        'logprob': -0.461602121591568},\n",
       "       {'token': 'ís',\n",
       "        'bytes': [195, 173, 115],\n",
       "        'logprob': -1.2116020917892456}]},\n",
       "     {'token': '\").',\n",
       "      'bytes': [34, 41, 46],\n",
       "      'logprob': -0.46065863966941833,\n",
       "      'top_logprobs': [{'token': '\").',\n",
       "        'bytes': [34, 41, 46],\n",
       "        'logprob': -0.46065863966941833},\n",
       "       {'token': '\"', 'bytes': [34], 'logprob': -1.8356586694717407}]}],\n",
       "    'refusal': None},\n",
       "   'message': {'content': 'The capital of France is باريس (in French: \"Pariser\").',\n",
       "    'role': 'assistant'}}],\n",
       " 'created': 1744207659,\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': 'fp_64e0ac9789',\n",
       " 'usage': {'completion_tokens': 15, 'prompt_tokens': 20, 'total_tokens': 35}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:08:20.500073Z",
     "start_time": "2025-04-20T13:08:12.095825Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_transformer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:09:09.055735Z",
     "start_time": "2025-04-20T13:09:09.046739Z"
    }
   },
   "source": [
    "print(tokenizer_transformer.encode(' Paris'))\n",
    "print(tokenizer_transformer.decode([12095]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49434, 239]\n",
      "[12095]\n",
      " Paris\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T15:47:02.039472Z",
     "start_time": "2025-04-07T15:47:00.067829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\":\"2023-10-06\",\"name\":\"Science Fair\",\"participants\":[\"Alice\",\"Bob\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T15:36:05.105230Z",
     "start_time": "2025-04-07T15:36:03.768294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-vzYwDXJ2qDLscvPv1RJ4pp54rDM07230', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': '{ \"winner\": \"Los Angeles Dodgers\" }', 'role': 'assistant'}}], 'created': 1744040175, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'usage': {'completion_tokens': 11, 'prompt_tokens': 41, 'total_tokens': 52}}\n",
      "{'content': '{ \"winner\": \"Los Angeles Dodgers\" }', 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020? Please respond in the format {winner: ...}\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "大模型并非全能, 而是在海量数据集下, 学会了\"猜\"出最可能的下一个词或句。数据集中某个领域知识越多， 它就猜的越准。\n",
    "\n",
    "![ai](../img/ai_and_huan.PNG)\n",
    "\n",
    "- 要深刻理解哪些是 AI 知道的, 哪些 AI 不知道的\n",
    "    - AI 知道: 简单讲\n",
    "        - 请你用鲁迅的文风帮我写一篇关于体育圈饭圈文化的文章\n",
    "        - 请用通俗易懂的语言帮我解释下祛魅这个词\n",
    "    - 人知道, AI 不知道的事情: 用投喂的方式\n",
    "        - B 站的辉耀计划视频风格文案 --> 举例子\n",
    "        - B 站何同学的微博发言 \n",
    "\n",
    "- 优化大模型手段\n",
    "    - 提示词工程\n",
    "    - 微调\n",
    "    - RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "- 结构化提示词\n",
    "    - 明确任务\n",
    "    - 补充背景\n",
    "    - 设定角色\n",
    "    - 给例子\n",
    "    - 规定格式\n",
    "- deepseek R1 推理模型技巧\n",
    "    - 比如: 我要 xx, 要给 xx 用, 希望达到 xx 效果, 但担心 xxx 问题\n",
    "    - 加入 说人话 提示词\n",
    "    - 多肯定，反复迭代追问多试几次\n",
    "- 升级提问, 逐级降维(不知道怎么提问, 直接询问 AI 如何提问)\n",
    "    - 我刚参加完一个会议，需要整理一份会议纪要。我不确定该从哪些方面入手，你能给我一些建议吗？\n",
    "    - 如何制定一份健身计划\n",
    "- reference\n",
    "    - 视频\n",
    "        - https://www.bilibili.com/video/BV1iuXkYHE2B\n",
    "        - https://www.bilibili.com/video/BV12JtpeiEvq\n",
    "    - 文档\n",
    "        - https://baoyu.io/blog/google-prompt-engineering-whitepaper\n",
    "        - https://dannyzheng.me/\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T15:04:09.250257Z",
     "start_time": "2025-04-12T15:03:57.825634Z"
    }
   },
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "content1 = \"在版本号命名规则中, 13.11 和 13.8 那个大?\"\n",
    "content2 = \"在数学中, 13.11 和 13.8 那个大?\"\n",
    "\n",
    "def get_response(model, content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    custom_model = \"deepseek-v3\"\n",
    "    # 并行发起两个请求\n",
    "    future1 = executor.submit(get_response, model,  content1)\n",
    "    future2 = executor.submit(get_response, model, content2)\n",
    "\n",
    "    response1 = future1.result()\n",
    "    response2 = future2.result()\n",
    "    print(response1.choices[0].message.content)\n",
    "    print(\"=\" * 100)\n",
    "    print(response2.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在标准的版本号命名规则（例如语义化版本 SemVer）中，版本号是按部分从左到右进行比较的。\n",
      "\n",
      "比较 13.11 和 13.8：\n",
      "\n",
      "1.  **比较第一部分（主版本号 Major）：** 都是 13，相等。\n",
      "2.  **比较第二部分（次版本号 Minor）：** 11 和 8。\n",
      "3.  **判断大小：** 因为 11 大于 8。\n",
      "\n",
      "所以，**13.11** 比 13.8 大（或者说是更新的版本）。\n",
      "====================================================================================================\n",
      "在数学中比较 13.11 和 13.8 的大小：\n",
      "\n",
      "1.  **比较整数部分：** 两个数的整数部分都是 13，相同。\n",
      "2.  **比较小数部分（从左到右）：**\n",
      "    *   比较小数点后第一位（十分位）：13.11 的是 1，13.8 的是 8。\n",
      "    *   因为 8 大于 1。\n",
      "\n",
      "所以，**13.8** 大于 13.11。\n",
      "\n",
      "简单来说，你可以把 13.8 看作 13.80。这样比较 13.11 和 13.80 就更直观了，因为 80 大于 11，所以 13.80 (即 13.8) 大于 13.11。\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以前在B站看视频，主播常常提醒要一键三连，我总是答应着“好好好”，但其实下了视频就忘记了。 这样其实不太好。现在如果主播说要一键三连，除非视频真的让我觉得很有意思、很有价值，否则我会诚实地告诉他们，抱歉我不打算三连，然后就离开。 作为一个喜欢支持创作者的人，这也是我锻炼诚实和勇气的方式。\n"
     ]
    }
   ],
   "source": [
    "content1 = \"\"\"<example> 以前打网约车，司机师傅跟我说打个好评，我都会说好好好，但是下车后也没想起来打。\n",
    "其实这样挺不好的。现在司机师傅跟我说打个好评，除非服务真的很好到我想打好评的程度，否则我就会直接说，抱歉我不想打，然后下车。\n",
    "作为一个有讨好倾向的人，这是我锻炼真诚和勇气的方式。</examle>\n",
    "仿照上面 example 标签中语句, 写一段b站看视频一键三连的话术\"\"\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content1}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 为什么需要泛型\n",
      "泛型使得编程更加灵活和可复用。它允许开发者编写可以处理多种数据类型的代码，从而避免了代码重复和强制类型转换的需求。同时，泛型可以提高类型安全性，降低运行时错误的可能性。\n",
      "\n",
      "### 什么是泛型\n",
      "泛型是一种编程语言特性，允许函数、类或接口在定义时不指定具体的数据类型，而是在具体使用时提供数据类型。这样，开发者可以创建可以适用于多种数据类型的代码。\n",
      "\n",
      "### 怎么使用泛型\n",
      "在使用泛型时，通常会在函数、类或接口的定义中引入类型参数。例如，在 Java 中，可以定义一个泛型类如下：\n",
      "\n",
      "```java\n",
      "public class Box<T> {\n",
      "    private T item;\n",
      "\n",
      "    public void setItem(T item) {\n",
      "        this.item = item;\n",
      "    }\n",
      "\n",
      "    public T getItem() {\n",
      "        return item;\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "使用时可以指定具体的数据类型：\n",
      "\n",
      "```java\n",
      "Box<Integer> integerBox = new Box<>();\n",
      "integerBox.setItem(123);\n",
      "Integer item = integerBox.getItem();\n",
      "```\n",
      "\n",
      "### 使用泛型时注意的问题\n",
      "1. **类型擦除**：在一些语言（例如 Java）中，泛型在运行时会被擦除为原始类型，需要注意类型信息可能无法在运行时获取。\n",
      "2. **不能使用基本类型**：一些语言不允许直接使用基本数据类型作为泛型参数，需使用对应的包装类。\n",
      "3. **有界参数**：如果需要限制泛型类型的范围，可以使用有界类型参数，但这可能增加复杂性。\n",
      "\n",
      "### 泛型的应用领域\n",
      "泛型广泛应用于数据结构（如集合、列表、树等）、算法（例如排序、搜索）和API设计中。许多现代编程语言（如 Java、C#、C++ 和 Swift）都支持泛型。\n",
      "\n",
      "### 泛型的优缺点\n",
      "**优点**：\n",
      "- 代码复用性高：支持多种数据类型，可以减少代码重复。\n",
      "- 类型安全性高：减少了因类型不匹配导致的运行时错误。\n",
      "- 提高可读性：代码更加简洁清晰，易于理解。\n",
      "\n",
      "**缺点**：\n",
      "- 学习曲线：初学者可能需要时间理解泛型的概念及其用法。\n",
      "- 复杂性增加：在一些情况下，泛型代码可能难以理解和调试。\n",
      "- 性能开销：某些实现可能导致性能降低，但这通常是微乎其微的。\n",
      "\n",
      "### 泛型是否重要, 是否需要花精力钻研\n",
      "对于初学者：了解泛型的基本概念和用法是非常重要的，它可以帮助提升编程能力和代码质量。建议初学者投入一定时间学习泛型，掌握其基本用法。\n",
      "\n",
      "对于专家：泛型是编写高效和可维护代码的关键工具。专家需要深入理解泛型的原理、应用及其在不同编程语言中的实现，以便在复杂项目中灵活应用。\n"
     ]
    }
   ],
   "source": [
    "content1 = \"\"\"<example>\n",
    "为什么需要 A\n",
    "什么是 A\n",
    "怎么使用 A\n",
    "使用 A 时注意的问题\n",
    "A 的应用领域\n",
    "A 的优缺点\n",
    "A 是否重要, 是否需要花精力钻研 (初学者, 专家级别)</example>\n",
    "请你按照上面 example 标签中思考逻辑, 回答编程语言中的 泛型\"\"\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content1}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response1.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
