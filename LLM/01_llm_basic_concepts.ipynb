{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型简介\n",
    "- 将互联网上的资料压缩在 LLM 中形成概率权重, 然后根据输入(prompt + output)预测下一个单词\n",
    "- output[n] = f(prompt + output[:n-1])\n",
    "- 入门文章\n",
    "  - https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "# 大模型基本参数\n",
    "- 基本对话\n",
    "- context size\n",
    "- tokenizer\n",
    "  - top_logprobs \n",
    "  - tempature\n",
    "  - logit_bias \n",
    "  - presence_penalty frequency_penalty \n",
    "- structure output\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:19:20.581544Z",
     "start_time": "2025-04-21T13:19:20.535544Z"
    }
   },
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "model = os.getenv(\"OPENAI_MODEL\")\n",
    "# print(client.base_url)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/gpt-4o\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:44:55.101947Z",
     "start_time": "2025-04-07T14:44:51.804059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6, 7, 8, 9, 10'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non stream\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    "    # temperature=0,\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context size\n",
    "- 产生的 token 越多, 模型运行过程中要的显存也就越大.\n",
    "    - xB -> 2x GB,  llama-8b -> 16GB, llama-1b -> 2GB\n",
    "    - [kv_cache_calculator](https://lmcache.ai/kv_cache_calculator.html)\n",
    "- [tokenizer_config.json](https://huggingface.co/Qwen/Qwen2.5-Omni-7B/blob/main/tokenizer_config.json) 中 model_max_length (context size)\n",
    "    - context size = input (prompt) + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer \n",
    "- https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:49:06.711397Z",
     "start_time": "2025-04-10T12:49:06.414398Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:49:10.728704Z",
     "start_time": "2025-04-10T12:49:10.717708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12650]\n",
      "[72782]\n",
      " Paris\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(' Paris'))\n",
    "print(tokenizer.encode('Paris'))\n",
    "print(tokenizer.decode([12650]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T13:08:20.500073Z",
     "start_time": "2025-04-20T13:08:12.095825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12095]\n",
      " Paris\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_transformer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "print(tokenizer_transformer.encode(' Paris'))\n",
    "print(tokenizer_transformer.decode([12095]))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:19:28.946249Z",
     "start_time": "2025-04-21T13:19:25.933190Z"
    }
   },
   "source": [
    "# top_logprobs & logit_bias\n",
    "resp1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'what is the capital of France?'}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=2\n",
    ")\n",
    "\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'what is the capital of France?'}\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=2,\n",
    "    temperature=0,\n",
    "    logit_bias={12650:-100, 72782:-100}\n",
    ")\n",
    "\n",
    "\n",
    "print(f'resp1 {resp1.choices[0].message.content}')\n",
    "print(f'resp2 {resp2.choices[0].message.content}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp1 The capital of France is Paris.\n",
      "resp2 The capital of France is **París**.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:22:18.790203Z",
     "start_time": "2025-04-21T13:22:18.773552Z"
    }
   },
   "source": [
    "resp2.choices[0].message.content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **París**.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:19:39.847655Z",
     "start_time": "2025-04-21T13:19:39.822657Z"
    }
   },
   "source": [
    "resp2.to_dict()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'gen-1745241569-nOmIOnNUy0Vs9PMRmG7M',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'The',\n",
       "      'bytes': [84, 104, 101],\n",
       "      'logprob': -1.2664456789934775e-06,\n",
       "      'top_logprobs': [{'token': 'The',\n",
       "        'bytes': [84, 104, 101],\n",
       "        'logprob': -1.2664456789934775e-06},\n",
       "       {'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': -13.750000953674316}]},\n",
       "     {'token': ' capital',\n",
       "      'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' capital',\n",
       "        'bytes': [32, 99, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' Capital',\n",
       "        'bytes': [32, 67, 97, 112, 105, 116, 97, 108],\n",
       "        'logprob': -25.0}]},\n",
       "     {'token': ' of',\n",
       "      'bytes': [32, 111, 102],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' of',\n",
       "        'bytes': [32, 111, 102],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' city',\n",
       "        'bytes': [32, 99, 105, 116, 121],\n",
       "        'logprob': -17.375}]},\n",
       "     {'token': ' France',\n",
       "      'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' France',\n",
       "        'bytes': [32, 70, 114, 97, 110, 99, 101],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'France',\n",
       "        'bytes': [70, 114, 97, 110, 99, 101],\n",
       "        'logprob': -25.875}]},\n",
       "     {'token': ' is',\n",
       "      'bytes': [32, 105, 115],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': [{'token': ' is',\n",
       "        'bytes': [32, 105, 115],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ',', 'bytes': [44], 'logprob': -23.9375}]},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': -22.875,\n",
       "      'top_logprobs': [{'token': ' Paris',\n",
       "        'bytes': [32, 80, 97, 114, 105, 115],\n",
       "        'logprob': 0.0},\n",
       "       {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -22.875}]},\n",
       "     {'token': 'Par',\n",
       "      'bytes': [80, 97, 114],\n",
       "      'logprob': -18.125,\n",
       "      'top_logprobs': [{'token': 'Paris',\n",
       "        'bytes': [80, 97, 114, 105, 115],\n",
       "        'logprob': 0.0},\n",
       "       {'token': 'Par', 'bytes': [80, 97, 114], 'logprob': -18.125}]},\n",
       "     {'token': 'ís',\n",
       "      'bytes': [195, 173, 115],\n",
       "      'logprob': -0.6560950875282288,\n",
       "      'top_logprobs': [{'token': 'ís',\n",
       "        'bytes': [195, 173, 115],\n",
       "        'logprob': -0.6560950875282288},\n",
       "       {'token': '<|end|>', 'bytes': None, 'logprob': -1.281095027923584}]},\n",
       "     {'token': '**',\n",
       "      'bytes': [42, 42],\n",
       "      'logprob': -0.002637130208313465,\n",
       "      'top_logprobs': [{'token': '**',\n",
       "        'bytes': [42, 42],\n",
       "        'logprob': -0.002637130208313465},\n",
       "       {'token': '.', 'bytes': [46], 'logprob': -6.002636909484863}]},\n",
       "     {'token': '.',\n",
       "      'bytes': [46],\n",
       "      'logprob': -0.0007167232106439769,\n",
       "      'top_logprobs': [{'token': '.',\n",
       "        'bytes': [46],\n",
       "        'logprob': -0.0007167232106439769},\n",
       "       {'token': ' (', 'bytes': [32, 40], 'logprob': -7.750716686248779}]}],\n",
       "    'refusal': []},\n",
       "   'message': {'content': 'The capital of France is **París**.',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'reasoning': None},\n",
       "   'native_finish_reason': 'stop'}],\n",
       " 'created': 1745241569,\n",
       " 'model': 'openai/gpt-4o',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': 'fp_f5bdcc3276',\n",
       " 'usage': {'completion_tokens': 11,\n",
       "  'prompt_tokens': 14,\n",
       "  'total_tokens': 25,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'cached_tokens': 0}},\n",
       " 'provider': 'OpenAI'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T13:22:44.577423Z",
     "start_time": "2025-04-21T13:22:33.007850Z"
    }
   },
   "source": [
    "content = '200个字描述一座山'\n",
    "\n",
    "resp1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content}\n",
    "    ],\n",
    ")\n",
    "\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content}\n",
    "    ],\n",
    "    presence_penalty=2,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "\n",
    "print(resp1.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "print(resp2.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在辽阔的平原尽头，屹立着一座巍峨壮丽的山峰。这座山被葱茏的森林和潺潺的小溪环绕，宛如大自然的杰作。清晨，薄雾笼罩山腰，宛如给它披上一层神秘的面纱。阳光洒在山顶，金光灿灿，仿佛镶嵌着宝石。山间鸟鸣婉转，松鼠在枝头蹿跳，增添了生机。山路蜿蜒盘旋，时而陡峭，时而平缓，吸引着无数探险者前来挑战。山顶视野开阔，远眺壮丽山河令人心旷神怡。这座山不仅是自然的奇观，更是无数人心灵的归宿。\n",
      "\n",
      "\n",
      "这座山，如同一位沉默的巨人，巍然屹立于天地之间。清晨时分，薄雾缭绕在山腰，不禁让人心生敬畏。在阳光照耀下，郁郁葱葱的植被披上了金色外衣，一片生机盎然。从远处眺望，这座山呈现出优美流畅的曲线，与湛蓝天空相映成趣。攀登其间，你可以听见潺潺溪水声与鸟鸣交织而成的大自然乐章，让每一步都充满诗意。当你终于踏上峰顶，那无垠视野和壮丽景观会冲刷掉一路上的疲惫，只留无限赞叹。这是一座美寓动静、融情于境之佳作，为历代文人与旅者留下不尽回味。\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T15:47:02.039472Z",
     "start_time": "2025-04-07T15:47:00.067829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\":\"2023-10-06\",\"name\":\"Science Fair\",\"participants\":[\"Alice\",\"Bob\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T15:36:05.105230Z",
     "start_time": "2025-04-07T15:36:03.768294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-vzYwDXJ2qDLscvPv1RJ4pp54rDM07230', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': '{ \"winner\": \"Los Angeles Dodgers\" }', 'role': 'assistant'}}], 'created': 1744040175, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'usage': {'completion_tokens': 11, 'prompt_tokens': 41, 'total_tokens': 52}}\n",
      "{'content': '{ \"winner\": \"Los Angeles Dodgers\" }', 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020? Please respond in the format {winner: ...}\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大模型并非全能, 而是在海量数据集下, 学会了\"猜\"出最可能的下一个词或句。数据集中某个领域知识越多， 它就猜的越准。\n",
    "\n",
    "![ai](../img/ai_and_huan.PNG)\n",
    "\n",
    "- 要深刻理解哪些是 AI 知道的, 哪些 AI 不知道的\n",
    "    - AI 知道: 简单讲\n",
    "        - 请你用鲁迅的文风帮我写一篇关于体育圈饭圈文化的文章\n",
    "        - 请用通俗易懂的语言帮我解释下祛魅这个词\n",
    "    - 人知道, AI 不知道的事情: 用投喂的方式\n",
    "        - B 站的辉耀计划视频风格文案 --> 举例子\n",
    "        - B 站何同学的微博发言 \n",
    "\n",
    "- 优化大模型手段\n",
    "    - 提示词工程\n",
    "    - 微调\n",
    "    - RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "- 结构化提示词\n",
    "    - 明确任务\n",
    "    - 补充背景\n",
    "    - 设定角色\n",
    "    - 给例子\n",
    "    - 规定格式\n",
    "- deepseek R1 推理模型技巧\n",
    "    - 比如: 我要 xx, 要给 xx 用, 希望达到 xx 效果, 但担心 xxx 问题\n",
    "    - 加入 说人话 提示词\n",
    "    - 多肯定，反复迭代追问多试几次\n",
    "- 升级提问, 逐级降维(不知道怎么提问, 直接询问 AI 如何提问)\n",
    "    - 我刚参加完一个会议，需要整理一份会议纪要。我不确定该从哪些方面入手，你能给我一些建议吗？\n",
    "    - 如何制定一份健身计划\n",
    "- reference\n",
    "    - 视频\n",
    "        - https://www.bilibili.com/video/BV1iuXkYHE2B\n",
    "        - https://www.bilibili.com/video/BV12JtpeiEvq\n",
    "    - 文档\n",
    "        - https://baoyu.io/blog/google-prompt-engineering-whitepaper\n",
    "        - https://dannyzheng.me/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T15:04:09.250257Z",
     "start_time": "2025-04-12T15:03:57.825634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在标准的版本号命名规则（例如语义化版本 SemVer）中，版本号是按部分从左到右进行比较的。\n",
      "\n",
      "比较 13.11 和 13.8：\n",
      "\n",
      "1.  **比较第一部分（主版本号 Major）：** 都是 13，相等。\n",
      "2.  **比较第二部分（次版本号 Minor）：** 11 和 8。\n",
      "3.  **判断大小：** 因为 11 大于 8。\n",
      "\n",
      "所以，**13.11** 比 13.8 大（或者说是更新的版本）。\n",
      "====================================================================================================\n",
      "在数学中比较 13.11 和 13.8 的大小：\n",
      "\n",
      "1.  **比较整数部分：** 两个数的整数部分都是 13，相同。\n",
      "2.  **比较小数部分（从左到右）：**\n",
      "    *   比较小数点后第一位（十分位）：13.11 的是 1，13.8 的是 8。\n",
      "    *   因为 8 大于 1。\n",
      "\n",
      "所以，**13.8** 大于 13.11。\n",
      "\n",
      "简单来说，你可以把 13.8 看作 13.80。这样比较 13.11 和 13.80 就更直观了，因为 80 大于 11，所以 13.80 (即 13.8) 大于 13.11。\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "content1 = \"在版本号命名规则中, 13.11 和 13.8 那个大?\"\n",
    "content2 = \"在数学中, 13.11 和 13.8 那个大?\"\n",
    "\n",
    "def get_response(model, content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    custom_model = \"deepseek-v3\"\n",
    "    # 并行发起两个请求\n",
    "    future1 = executor.submit(get_response, model,  content1)\n",
    "    future2 = executor.submit(get_response, model, content2)\n",
    "\n",
    "    response1 = future1.result()\n",
    "    response2 = future2.result()\n",
    "    print(response1.choices[0].message.content)\n",
    "    print(\"=\" * 100)\n",
    "    print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以前在B站看视频，主播常常提醒要一键三连，我总是答应着“好好好”，但其实下了视频就忘记了。 这样其实不太好。现在如果主播说要一键三连，除非视频真的让我觉得很有意思、很有价值，否则我会诚实地告诉他们，抱歉我不打算三连，然后就离开。 作为一个喜欢支持创作者的人，这也是我锻炼诚实和勇气的方式。\n"
     ]
    }
   ],
   "source": [
    "content1 = \"\"\"<example> 以前打网约车，司机师傅跟我说打个好评，我都会说好好好，但是下车后也没想起来打。\n",
    "其实这样挺不好的。现在司机师傅跟我说打个好评，除非服务真的很好到我想打好评的程度，否则我就会直接说，抱歉我不想打，然后下车。\n",
    "作为一个有讨好倾向的人，这是我锻炼真诚和勇气的方式。</examle>\n",
    "仿照上面 example 标签中语句, 写一段b站看视频一键三连的话术\"\"\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content1}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 为什么需要泛型\n",
      "泛型使得编程更加灵活和可复用。它允许开发者编写可以处理多种数据类型的代码，从而避免了代码重复和强制类型转换的需求。同时，泛型可以提高类型安全性，降低运行时错误的可能性。\n",
      "\n",
      "### 什么是泛型\n",
      "泛型是一种编程语言特性，允许函数、类或接口在定义时不指定具体的数据类型，而是在具体使用时提供数据类型。这样，开发者可以创建可以适用于多种数据类型的代码。\n",
      "\n",
      "### 怎么使用泛型\n",
      "在使用泛型时，通常会在函数、类或接口的定义中引入类型参数。例如，在 Java 中，可以定义一个泛型类如下：\n",
      "\n",
      "```java\n",
      "public class Box<T> {\n",
      "    private T item;\n",
      "\n",
      "    public void setItem(T item) {\n",
      "        this.item = item;\n",
      "    }\n",
      "\n",
      "    public T getItem() {\n",
      "        return item;\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "使用时可以指定具体的数据类型：\n",
      "\n",
      "```java\n",
      "Box<Integer> integerBox = new Box<>();\n",
      "integerBox.setItem(123);\n",
      "Integer item = integerBox.getItem();\n",
      "```\n",
      "\n",
      "### 使用泛型时注意的问题\n",
      "1. **类型擦除**：在一些语言（例如 Java）中，泛型在运行时会被擦除为原始类型，需要注意类型信息可能无法在运行时获取。\n",
      "2. **不能使用基本类型**：一些语言不允许直接使用基本数据类型作为泛型参数，需使用对应的包装类。\n",
      "3. **有界参数**：如果需要限制泛型类型的范围，可以使用有界类型参数，但这可能增加复杂性。\n",
      "\n",
      "### 泛型的应用领域\n",
      "泛型广泛应用于数据结构（如集合、列表、树等）、算法（例如排序、搜索）和API设计中。许多现代编程语言（如 Java、C#、C++ 和 Swift）都支持泛型。\n",
      "\n",
      "### 泛型的优缺点\n",
      "**优点**：\n",
      "- 代码复用性高：支持多种数据类型，可以减少代码重复。\n",
      "- 类型安全性高：减少了因类型不匹配导致的运行时错误。\n",
      "- 提高可读性：代码更加简洁清晰，易于理解。\n",
      "\n",
      "**缺点**：\n",
      "- 学习曲线：初学者可能需要时间理解泛型的概念及其用法。\n",
      "- 复杂性增加：在一些情况下，泛型代码可能难以理解和调试。\n",
      "- 性能开销：某些实现可能导致性能降低，但这通常是微乎其微的。\n",
      "\n",
      "### 泛型是否重要, 是否需要花精力钻研\n",
      "对于初学者：了解泛型的基本概念和用法是非常重要的，它可以帮助提升编程能力和代码质量。建议初学者投入一定时间学习泛型，掌握其基本用法。\n",
      "\n",
      "对于专家：泛型是编写高效和可维护代码的关键工具。专家需要深入理解泛型的原理、应用及其在不同编程语言中的实现，以便在复杂项目中灵活应用。\n"
     ]
    }
   ],
   "source": [
    "content1 = \"\"\"<example>\n",
    "为什么需要 A\n",
    "什么是 A\n",
    "怎么使用 A\n",
    "使用 A 时注意的问题\n",
    "A 的应用领域\n",
    "A 的优缺点\n",
    "A 是否重要, 是否需要花精力钻研 (初学者, 专家级别)</example>\n",
    "请你按照上面 example 标签中思考逻辑, 回答编程语言中的 泛型\"\"\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': content1}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response1.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
